{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bdAi0Q4sipgb"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(seed)\n",
        "\n",
        "import logging\n",
        "\n",
        "import random\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aM6CRdWkipgc",
        "outputId": "ca48f09b-2939-48b9-c018-50addc712c0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.15.0\n"
          ]
        }
      ],
      "source": [
        "# Import tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
        "\n",
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6RkjTxlxipgd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pandas import Series\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "plt.rc('font', size=16)\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive/HW2"
      ],
      "metadata": {
        "id": "hfQHThGDi-vl",
        "outputId": "bb4d60b0-7d49-49bb-c65a-892afb0447d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive/My Drive/HW2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Cy-6W_X8ipge",
        "outputId": "08c2ad1e-9c4e-44a8-8254-de238cb6fb01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(48000, 2776)\n",
            "(48000, 2)\n",
            "(48000,)\n"
          ]
        }
      ],
      "source": [
        "training_data = np.load(\"training_data.npy\").astype(np.float32)\n",
        "categories = np.load(\"categories.npy\")\n",
        "valid_periods=np.load(\"valid_periods.npy\")\n",
        "categories_unique = np.unique(categories)\n",
        "\n",
        "print(training_data.shape)\n",
        "print(valid_periods.shape)\n",
        "print(categories.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ujez38bmipge"
      },
      "outputs": [],
      "source": [
        "def build_dataset(category, window_length=200, stride=25):\n",
        "    dataset = []\n",
        "\n",
        "    skipped_samples = 0\n",
        "    dataset_window_length = window_length\n",
        "    sample_map = categories==category\n",
        "\n",
        "    valid_perios_for_category = valid_periods[sample_map]\n",
        "    training_data_for_category = training_data[sample_map]\n",
        "\n",
        "    for i in range(len(valid_perios_for_category)):\n",
        "        from_time = valid_perios_for_category[i,0]\n",
        "        to_time = valid_perios_for_category[i,1]\n",
        "        duration = to_time - from_time\n",
        "\n",
        "        if(duration<dataset_window_length):\n",
        "            skipped_samples+=1\n",
        "            continue\n",
        "\n",
        "        n_samples = 1 + (duration-dataset_window_length)//stride\n",
        "\n",
        "        for j in range(n_samples):\n",
        "            sample = training_data_for_category[i][from_time+j*stride:from_time+j*stride+dataset_window_length]\n",
        "            dataset.append(sample)\n",
        "\n",
        "    dataset = np.array(dataset)\n",
        "    dataset = dataset.reshape((dataset.shape[0], dataset.shape[1], 1))\n",
        "    print(\"Skipped samples: \", skipped_samples)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "def build_dataset_zero_padding(category, window=200, stride=50):\n",
        "    dataset = []\n",
        "    sample_map = categories==category\n",
        "\n",
        "    periods = valid_periods[sample_map]\n",
        "    data = training_data[sample_map]\n",
        "\n",
        "    # Sanity check to avoid runtime errors\n",
        "    assert window % stride == 0\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        # Take only meaningful features\n",
        "        temp = data[i][periods[i, 0] : periods[i, 1]]\n",
        "        # Compute padding length\n",
        "        padding_len = window - len(temp)%window\n",
        "        # Create padding and concatenate it\n",
        "        padding = np.zeros(padding_len, dtype='float32')\n",
        "        temp = np.concatenate((padding,temp))\n",
        "        # Build features windows with their corresponging labels\n",
        "        idx = 0\n",
        "        while idx+window <= len(temp):\n",
        "            dataset.append(temp[idx:idx+window])\n",
        "            idx += stride\n",
        "    dataset = np.array(dataset)\n",
        "    dataset = dataset.reshape((dataset.shape[0], dataset.shape[1], 1))\n",
        "    return dataset\n",
        "\n",
        "def build_dataset_mean_padding(category, window=200, stride=50):\n",
        "    dataset = []\n",
        "    sample_map = categories==category\n",
        "\n",
        "    periods = valid_periods[sample_map]\n",
        "    data = training_data[sample_map]\n",
        "\n",
        "    # Sanity check to avoid runtime errors\n",
        "    assert window % stride == 0\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        # Take only meaningful features\n",
        "        temp = data[i][periods[i, 0] : periods[i, 1]]\n",
        "        # Compute padding length\n",
        "        padding_len = window - len(temp)%window\n",
        "        # Compute mean\n",
        "        mean = np.mean(temp)\n",
        "        # Create padding and concatenate it\n",
        "        padding = np.full(padding_len, mean, dtype='float32')\n",
        "        temp = np.concatenate((padding, temp))\n",
        "        # Build features windows with their corresponging labels\n",
        "        idx = 0\n",
        "        while idx+window <= len(temp):\n",
        "            dataset.append(temp[idx:idx+window])\n",
        "            idx += stride\n",
        "    dataset = np.array(dataset)\n",
        "    dataset = dataset.reshape((dataset.shape[0], dataset.shape[1], 1))\n",
        "    return dataset\n",
        "\n",
        "def build_dataset_mean_padding(category, window=200, stride=50):\n",
        "    dataset = []\n",
        "    sample_map = categories==category\n",
        "\n",
        "    periods = valid_periods[sample_map]\n",
        "    data = training_data[sample_map]\n",
        "\n",
        "    # Sanity check to avoid runtime errors\n",
        "    assert window % stride == 0\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        # Take only meaningful features\n",
        "        temp = data[i][periods[i, 0] : periods[i, 1]]\n",
        "        # Compute padding length\n",
        "        padding_len = window - len(temp)%window\n",
        "        # Compute mean\n",
        "        mean = np.mean(temp)\n",
        "        # Create padding and concatenate it\n",
        "        padding = np.full(padding_len, mean, dtype='float32')\n",
        "        temp = np.concatenate((padding, temp))\n",
        "        # Build features windows with their corresponging labels\n",
        "        idx = 0\n",
        "        while idx+window <= len(temp):\n",
        "            dataset.append(temp[idx:idx+window])\n",
        "            idx += stride\n",
        "    dataset = np.array(dataset)\n",
        "    dataset = dataset.reshape((dataset.shape[0], dataset.shape[1], 1))\n",
        "    return dataset\n",
        "\n",
        "def build_dataset_value_padding(category, window=200, stride=50, value=None):\n",
        "    dataset = []\n",
        "    sample_map = categories==category\n",
        "\n",
        "    periods = valid_periods[sample_map]\n",
        "    data = training_data[sample_map]\n",
        "\n",
        "    # Sanity check to avoid runtime errors\n",
        "    assert window % stride == 0\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        # Take only meaningful features\n",
        "        temp = data[i][periods[i, 0] : periods[i, 1]]\n",
        "        # Compute padding length\n",
        "        padding_len = window - len(temp)%window\n",
        "        # Compute mean\n",
        "        padding_value = value\n",
        "        if value is None:\n",
        "            padding_value = temp[0]\n",
        "        # Create padding and concatenate it\n",
        "        padding = np.full(padding_len, padding_value, dtype='float32')\n",
        "        temp = np.concatenate((padding, temp))\n",
        "        # Build features windows with their corresponging labels\n",
        "        idx = 0\n",
        "        while idx+window <= len(temp):\n",
        "            dataset.append(temp[idx:idx+window])\n",
        "            idx += stride\n",
        "    dataset = np.array(dataset)\n",
        "    dataset = dataset.reshape((dataset.shape[0], dataset.shape[1], 1))\n",
        "    return dataset\n",
        "\n",
        "def build_dataset_repeat_padding(category, window=200, stride=50):\n",
        "    dataset = []\n",
        "    sample_map = categories==category\n",
        "\n",
        "    periods = valid_periods[sample_map]\n",
        "    data = training_data[sample_map]\n",
        "\n",
        "    # Sanity check to avoid runtime errors\n",
        "    assert window % stride == 0\n",
        "\n",
        "    for i in range(len(data)):\n",
        "        # Take only meaningful features\n",
        "        temp = data[i][periods[i, 0] : periods[i, 1]]\n",
        "        # Compute padding length\n",
        "        padding_len = window - len(temp)%window\n",
        "        # Create padding and concatenate it\n",
        "        padding = temp[:padding_len]\n",
        "        temp = np.concatenate((padding, temp))\n",
        "        # Build features windows with their corresponging labels\n",
        "        idx = 0\n",
        "        while idx+window <= len(temp):\n",
        "            dataset.append(temp[idx:idx+window])\n",
        "            idx += stride\n",
        "    dataset = np.array(dataset)\n",
        "    dataset = dataset.reshape((dataset.shape[0], dataset.shape[1], 1))\n",
        "    return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cwYL-qwlipge"
      },
      "outputs": [],
      "source": [
        "window_length=[20, 80, 20, 80, 20]\n",
        "telescope=10\n",
        "stride=[5, 20, 5, 20, 5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MhoFMoxFipgf",
        "outputId": "90464940-5b79-4ba4-b703-4e3a4321aa7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped samples:  0\n",
            "Skipped samples:  3683\n",
            "Skipped samples:  0\n",
            "Skipped samples:  2663\n",
            "Skipped samples:  0\n"
          ]
        }
      ],
      "source": [
        "datasets = {}\n",
        "for category, w, s in zip(categories_unique, window_length, stride):\n",
        "    dataset = build_dataset(category, window_length=w, stride=s)\n",
        "    datasets[category] = dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "source": [
        "# Standardize data\n",
        "\n",
        "standardize = False\n",
        "\n",
        "if standardize:\n",
        "    for category in categories_unique:\n",
        "        scaler = StandardScaler()\n",
        "\n",
        "        data = []\n",
        "        sample_map = categories==category\n",
        "\n",
        "        periods = valid_periods[sample_map]\n",
        "        ts = training_data[sample_map]\n",
        "        for t, v in zip(ts, periods):\n",
        "            data += t[v[0]:v[1]].flatten().tolist()\n",
        "\n",
        "        series = Series(data)\n",
        "        values = series.values\n",
        "        values = values.reshape((len(values), 1))\n",
        "        scaler = scaler.fit(values)\n",
        "\n",
        "        dataset = datasets[category]\n",
        "        for i in range(len(dataset)):\n",
        "            dataset[i] = scaler.transform(dataset[i])\n"
      ],
      "metadata": {
        "id": "ij-vDcmmipgf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iqlx9EWlipgf"
      },
      "outputs": [],
      "source": [
        "def build_CONV_LSTM_model(input_shape, output_shape):\n",
        "    # Ensure the input time steps are at least as many as the output time steps\n",
        "    assert input_shape[0] >= output_shape[0], \"For this exercise we want input time steps to be >= of output time steps\"\n",
        "\n",
        "    # Define the input layer with the specified shape\n",
        "    input_layer = tfkl.Input(shape=input_shape, name='input_layer')\n",
        "\n",
        "    # Add a Bidirectional LSTM layer with 64 units\n",
        "    x = tfkl.Bidirectional(tfkl.LSTM(64, return_sequences=True, name='lstm'), name='bidirectional_lstm')(input_layer)\n",
        "\n",
        "    # Add a 1D Convolution layer with 128 filters and a kernel size of 3\n",
        "    x = tfkl.Conv1D(128, 3, padding='same', activation='relu', name='conv')(x)\n",
        "\n",
        "    # Add a final Convolution layer to match the desired output shape\n",
        "    output_layer = tfkl.Conv1D(output_shape[1], 3, padding='same', name='output_layer')(x)\n",
        "\n",
        "    # Calculate the size to crop from the output to match the output shape\n",
        "    crop_size = output_layer.shape[1] - output_shape[0]\n",
        "\n",
        "    # Crop the output to the desired length\n",
        "    output_layer = tfkl.Cropping1D((0, crop_size), name='cropping')(output_layer)\n",
        "\n",
        "    # Construct the model by connecting input and output layers\n",
        "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer, name='CONV_LSTM_model')\n",
        "\n",
        "    # Compile the model with Mean Squared Error loss and Adam optimizer\n",
        "    model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=tf.keras.optimizers.Adam(), metrics=['mse', \"mae\"])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "t6MveqFTipgg"
      },
      "outputs": [],
      "source": [
        "def train_models(datasets, testOne=None):\n",
        "    histories = {}\n",
        "    for category in (categories_unique if testOne is None else categories_unique[testOne]):\n",
        "        print(\"Training model for category: \", category)\n",
        "        X_data = datasets[category]\n",
        "        X_train, y_train = X_data[:, :-telescope], X_data[:, -telescope:]\n",
        "\n",
        "        input_shape = X_train.shape[1:]\n",
        "        output_shape = y_train.shape[1:]\n",
        "        batch_size = 64\n",
        "        epochs = 200\n",
        "\n",
        "        model = build_CONV_LSTM_model(input_shape, output_shape)\n",
        "        history = model.fit(\n",
        "            x = X_train,\n",
        "            y = y_train,\n",
        "            batch_size = batch_size,\n",
        "            epochs = epochs,\n",
        "            validation_split=.1,\n",
        "            callbacks = [\n",
        "                tfk.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=12, restore_best_weights=True),\n",
        "                tfk.callbacks.ReduceLROnPlateau(monitor='val_loss', mode='min', patience=10, factor=0.1, min_lr=1e-5)\n",
        "            ]\n",
        "        ).history\n",
        "\n",
        "        histories[category] = history\n",
        "\n",
        "        model.save(\"Vanilla_200Window/Vanilla_\"+category)\n",
        "        del model\n",
        "\n",
        "    for c in categories_unique:\n",
        "        print()\n",
        "        print(\"Model \" + category + \" MSE: \"+ str(histories[c][\"val_mse\"][-1]))\n",
        "        print(\"Model \" + category + \" MAE: \" + str(histories[c][\"val_mae\"][-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klvY_fwMipgg",
        "outputId": "378269d4-a038-44fd-ce71-10fa3a9a3c5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for category:  A\n",
            "Epoch 1/200\n",
            "4198/4198 [==============================] - 34s 7ms/step - loss: 0.0115 - mse: 0.0115 - mae: 0.0747 - val_loss: 0.0130 - val_mse: 0.0130 - val_mae: 0.0796 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0106 - mse: 0.0106 - mae: 0.0712 - val_loss: 0.0126 - val_mse: 0.0126 - val_mae: 0.0791 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "4198/4198 [==============================] - 32s 8ms/step - loss: 0.0103 - mse: 0.0103 - mae: 0.0699 - val_loss: 0.0123 - val_mse: 0.0123 - val_mae: 0.0768 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0101 - mse: 0.0101 - mae: 0.0691 - val_loss: 0.0120 - val_mse: 0.0120 - val_mae: 0.0769 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "4198/4198 [==============================] - 31s 7ms/step - loss: 0.0100 - mse: 0.0100 - mae: 0.0687 - val_loss: 0.0120 - val_mse: 0.0120 - val_mae: 0.0770 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0100 - mse: 0.0100 - mae: 0.0683 - val_loss: 0.0125 - val_mse: 0.0125 - val_mae: 0.0786 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "4198/4198 [==============================] - 33s 8ms/step - loss: 0.0099 - mse: 0.0099 - mae: 0.0680 - val_loss: 0.0118 - val_mse: 0.0118 - val_mae: 0.0749 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0098 - mse: 0.0098 - mae: 0.0678 - val_loss: 0.0127 - val_mse: 0.0127 - val_mae: 0.0801 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0098 - mse: 0.0098 - mae: 0.0676 - val_loss: 0.0119 - val_mse: 0.0119 - val_mae: 0.0751 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0098 - mse: 0.0098 - mae: 0.0675 - val_loss: 0.0116 - val_mse: 0.0116 - val_mae: 0.0743 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "4198/4198 [==============================] - 31s 7ms/step - loss: 0.0097 - mse: 0.0097 - mae: 0.0673 - val_loss: 0.0117 - val_mse: 0.0117 - val_mae: 0.0738 - lr: 0.0010\n",
            "Epoch 12/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0097 - mse: 0.0097 - mae: 0.0672 - val_loss: 0.0117 - val_mse: 0.0117 - val_mae: 0.0747 - lr: 0.0010\n",
            "Epoch 13/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0097 - mse: 0.0097 - mae: 0.0671 - val_loss: 0.0117 - val_mse: 0.0117 - val_mae: 0.0746 - lr: 0.0010\n",
            "Epoch 14/200\n",
            "4198/4198 [==============================] - 29s 7ms/step - loss: 0.0097 - mse: 0.0097 - mae: 0.0670 - val_loss: 0.0116 - val_mse: 0.0116 - val_mae: 0.0739 - lr: 0.0010\n",
            "Epoch 15/200\n",
            "4198/4198 [==============================] - 31s 7ms/step - loss: 0.0096 - mse: 0.0096 - mae: 0.0669 - val_loss: 0.0120 - val_mse: 0.0120 - val_mae: 0.0771 - lr: 0.0010\n",
            "Epoch 16/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0096 - mse: 0.0096 - mae: 0.0669 - val_loss: 0.0117 - val_mse: 0.0117 - val_mae: 0.0741 - lr: 0.0010\n",
            "Epoch 17/200\n",
            "4198/4198 [==============================] - 31s 7ms/step - loss: 0.0096 - mse: 0.0096 - mae: 0.0667 - val_loss: 0.0118 - val_mse: 0.0118 - val_mae: 0.0741 - lr: 0.0010\n",
            "Epoch 18/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0096 - mse: 0.0096 - mae: 0.0667 - val_loss: 0.0117 - val_mse: 0.0117 - val_mae: 0.0739 - lr: 0.0010\n",
            "Epoch 19/200\n",
            "4198/4198 [==============================] - 31s 7ms/step - loss: 0.0096 - mse: 0.0096 - mae: 0.0666 - val_loss: 0.0114 - val_mse: 0.0114 - val_mae: 0.0738 - lr: 0.0010\n",
            "Epoch 20/200\n",
            "4198/4198 [==============================] - 29s 7ms/step - loss: 0.0095 - mse: 0.0095 - mae: 0.0665 - val_loss: 0.0116 - val_mse: 0.0116 - val_mae: 0.0741 - lr: 0.0010\n",
            "Epoch 21/200\n",
            "4198/4198 [==============================] - 31s 7ms/step - loss: 0.0095 - mse: 0.0095 - mae: 0.0665 - val_loss: 0.0116 - val_mse: 0.0116 - val_mae: 0.0742 - lr: 0.0010\n",
            "Epoch 22/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0095 - mse: 0.0095 - mae: 0.0664 - val_loss: 0.0116 - val_mse: 0.0116 - val_mae: 0.0746 - lr: 0.0010\n",
            "Epoch 23/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0095 - mse: 0.0095 - mae: 0.0663 - val_loss: 0.0115 - val_mse: 0.0115 - val_mae: 0.0736 - lr: 0.0010\n",
            "Epoch 24/200\n",
            "4198/4198 [==============================] - 31s 7ms/step - loss: 0.0095 - mse: 0.0095 - mae: 0.0662 - val_loss: 0.0115 - val_mse: 0.0115 - val_mae: 0.0741 - lr: 0.0010\n",
            "Epoch 25/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0094 - mse: 0.0094 - mae: 0.0661 - val_loss: 0.0115 - val_mse: 0.0115 - val_mae: 0.0738 - lr: 0.0010\n",
            "Epoch 26/200\n",
            "4198/4198 [==============================] - 31s 7ms/step - loss: 0.0094 - mse: 0.0094 - mae: 0.0661 - val_loss: 0.0116 - val_mse: 0.0116 - val_mae: 0.0737 - lr: 0.0010\n",
            "Epoch 27/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0094 - mse: 0.0094 - mae: 0.0661 - val_loss: 0.0118 - val_mse: 0.0118 - val_mae: 0.0752 - lr: 0.0010\n",
            "Epoch 28/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0094 - mse: 0.0094 - mae: 0.0660 - val_loss: 0.0115 - val_mse: 0.0115 - val_mae: 0.0734 - lr: 0.0010\n",
            "Epoch 29/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0094 - mse: 0.0094 - mae: 0.0659 - val_loss: 0.0114 - val_mse: 0.0114 - val_mae: 0.0733 - lr: 0.0010\n",
            "Epoch 30/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0092 - mse: 0.0092 - mae: 0.0650 - val_loss: 0.0113 - val_mse: 0.0113 - val_mae: 0.0726 - lr: 1.0000e-04\n",
            "Epoch 31/200\n",
            "4198/4198 [==============================] - 32s 8ms/step - loss: 0.0092 - mse: 0.0092 - mae: 0.0649 - val_loss: 0.0113 - val_mse: 0.0113 - val_mae: 0.0730 - lr: 1.0000e-04\n",
            "Epoch 32/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0092 - mse: 0.0092 - mae: 0.0649 - val_loss: 0.0114 - val_mse: 0.0114 - val_mae: 0.0730 - lr: 1.0000e-04\n",
            "Epoch 33/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0092 - mse: 0.0092 - mae: 0.0649 - val_loss: 0.0113 - val_mse: 0.0113 - val_mae: 0.0725 - lr: 1.0000e-04\n",
            "Epoch 34/200\n",
            "4198/4198 [==============================] - 31s 7ms/step - loss: 0.0092 - mse: 0.0092 - mae: 0.0648 - val_loss: 0.0113 - val_mse: 0.0113 - val_mae: 0.0727 - lr: 1.0000e-04\n",
            "Epoch 35/200\n",
            "4198/4198 [==============================] - 31s 7ms/step - loss: 0.0092 - mse: 0.0092 - mae: 0.0648 - val_loss: 0.0113 - val_mse: 0.0113 - val_mae: 0.0727 - lr: 1.0000e-04\n",
            "Epoch 36/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0092 - mse: 0.0092 - mae: 0.0648 - val_loss: 0.0113 - val_mse: 0.0113 - val_mae: 0.0725 - lr: 1.0000e-04\n",
            "Epoch 37/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0092 - mse: 0.0092 - mae: 0.0648 - val_loss: 0.0114 - val_mse: 0.0114 - val_mae: 0.0732 - lr: 1.0000e-04\n",
            "Epoch 38/200\n",
            "4198/4198 [==============================] - 31s 7ms/step - loss: 0.0091 - mse: 0.0091 - mae: 0.0648 - val_loss: 0.0113 - val_mse: 0.0113 - val_mae: 0.0725 - lr: 1.0000e-04\n",
            "Epoch 39/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0091 - mse: 0.0091 - mae: 0.0648 - val_loss: 0.0113 - val_mse: 0.0113 - val_mae: 0.0726 - lr: 1.0000e-04\n",
            "Epoch 40/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0091 - mse: 0.0091 - mae: 0.0648 - val_loss: 0.0114 - val_mse: 0.0114 - val_mae: 0.0732 - lr: 1.0000e-04\n",
            "Epoch 41/200\n",
            "4198/4198 [==============================] - 31s 7ms/step - loss: 0.0091 - mse: 0.0091 - mae: 0.0647 - val_loss: 0.0113 - val_mse: 0.0113 - val_mae: 0.0725 - lr: 1.0000e-05\n",
            "Epoch 42/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0091 - mse: 0.0091 - mae: 0.0647 - val_loss: 0.0113 - val_mse: 0.0113 - val_mae: 0.0725 - lr: 1.0000e-05\n",
            "Epoch 43/200\n",
            "4198/4198 [==============================] - 29s 7ms/step - loss: 0.0091 - mse: 0.0091 - mae: 0.0646 - val_loss: 0.0113 - val_mse: 0.0113 - val_mae: 0.0726 - lr: 1.0000e-05\n",
            "Epoch 44/200\n",
            "4198/4198 [==============================] - 30s 7ms/step - loss: 0.0091 - mse: 0.0091 - mae: 0.0646 - val_loss: 0.0113 - val_mse: 0.0113 - val_mae: 0.0726 - lr: 1.0000e-05\n",
            "Epoch 45/200\n",
            "2518/4198 [================>.............] - ETA: 11s - loss: 0.0091 - mse: 0.0091 - mae: 0.0647"
          ]
        }
      ],
      "source": [
        "train_models(datasets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWXXy_Gzipgg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}